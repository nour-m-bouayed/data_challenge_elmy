# Models and results

For the most part of this project we used quite standard tree based models, as they are known to perform well on tabular data. We also tried a few other models, such as a neural network and a support vector machine which presented some interesting results but not improved much our baseline. We would like to stress that the main work of this project, which also brought the most significant improvements, was the feature engineering and the data augmentation. We will present the different models we tried and the results we obtained.

## Lagged features
Our most successful approach was to use gradient boosting regression tree models with lagged features. We used the `xgboost` library as well as the `scikit-learn` implementation of Histogram-based Gradient Boosting Classification Tree model inspired by 'LightGBM'. Taking inspiration from \cite{elsayed2021we} we used lagged features to capture the temporal dependencies in the data. For each of the provided features of the original dataset we created 24 lagged features corresponding to the observations within 24 hours of the time of interest. We then augmented the input of the model with statistics(mean, standard deviation) of each of these features within the same time frame. Finally, we added the external features described in the previous section.

## Loss and metric
The metric used to evaluate the performance of the model is the Weighted Accuracy. This metric is defined as the proportion of predictions whose sign is correctly identified weighted by the absolute value of the observed differences. This metric is useful in the context of price difference prediction as only the sign of the difference is important, and the magnitude of the difference is not as relevant. However, in the case of large variations, as we can observe in our dataset, we want to make sure that the model is correct to avoid a large loss or not miss a large gain. In the type of model we used the most, Gradient Boosting Tree model, it may prove difficult, although theoretically possible to optimize directly for this metric as optimization were made with the mean absolute difference or the logistic loss. We choose instead to use proxy metrics, such as root-mean-square error in the case of regression, to perform the optimization of the model and then use the Weighted Accuracy for model selection and evaluation. Such losses minimize the error or maximize the accuracy which allows to have a good model that also performs well on the metric of interest.

## Classification variant
